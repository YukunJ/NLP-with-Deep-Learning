Assignment 4 written part 1
Yukun Jiang 

1. [Neural Machine Translation with RNNs]
1(g). 
	Essentially, in the step function, we manually set the attention score of "padded" to negative infinite, so that the "padded" don't get any attention in the process. And their information won't be passed to generate next step's output
	This is important because, if there are a lot very sentence and a very short sentence in the same batch, the short sentence will have many "padded"s and if we don't manually turn off their attention score, the many "padded" will share and hold a lot of overall attention but they are useless in the overall translation task. 

1(i).
	(I don't have Stanford Azure account, so I use free google colab GPU instead to train
	But google colab will disconnect after a while, so I cannot train the model fully
	I train for about 2 hours on colab free GPU till epoch 5, iter 30000, avg. loss 36.08, see the model.bin for the model storage)
	(Updates: I use local version to train for a long while, finally at epoch 14, iter 90000, cum. loss 27.82 and the training process stops)

	The Corpus BLEU: 21.410535016953933, indeed higher than 21.

1(j).
	Compare multiplicative attention with additive attention, multiplicative attention is faster in computation and space-efficient because of its usage of matrix multiplication. But additive attention might perform better because it balances both the attention at this time step and the current hidden state output.
 
2. [Analyzing NMT Systems]
a.
(i) 
Error: "... another favorite of my favorites ..."
Possible reason: In the Spanish sentence, "otro" and "favorito" spell quite similarly, so NMT might translate "otro" to "favorite" in English.
Fix: let the NMT learn more about the common phrase "one of the (plurals noun)"

(ii)
Error: "... I am probably the author for children, more reading in the U.S. ..."
Possible reason: The sentence and words ordering in Spanish confuse our NMT
Fix: using multiple stacked RNN/LSTM, or try more orders of processing instead of just forward and backward

(iii)
Error: "... Richard <unk> ..."
Possible reason: NMT doesn't recognize the new word "Bolingbroke"
Fix: When an unknown word encountered during translation, just keep it intact. 

(iv)
Error: "... get back to the apple  ..."
Possible reason: NMT cannot understand the Spanish Idioms
Fix: Feed more such commonly-used Spanish Idioms in the training

(v)
Error: "... women's room ... "
Possible reason: May the "she" in the beginning suggests a female-context and the "profesores" is not very gender-clear, the attention memorizes the long-context of "she"
Fix: Change the attention mechanism to filter out long-context irrelevant information

(vi)
Error: "... 100,000 acres ..."
Possible reason: NMT doesn't know how to do the proper unit conversion
Fix: insert certain pre-defined math rule for measurement conversion, etc.


